{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOverfitting: Overfitting occurs when a machine learning model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns. As a result, the model performs very well on the training data but poorly on unseen data, as it fails to generalize.\\n\\nConsequences: Poor generalization, high variance, model fails to perform on new data.\\n\\nMitigation: Reduce model complexity, gather more training data, use regularization techniques.\\n\\nUnderfitting: Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. It performs poorly on both the training and unseen data.\\n\\nConsequences: Poor performance, low accuracy on training and test data.\\n\\nMitigation: Increase model complexity, add relevant features, use more advanced algorithms.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''\n",
    "Overfitting: Overfitting occurs when a machine learning model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns. As a result, the model performs very well on the training data but poorly on unseen data, as it fails to generalize.\n",
    "\n",
    "Consequences: Poor generalization, high variance, model fails to perform on new data.\n",
    "\n",
    "Mitigation: Reduce model complexity, gather more training data, use regularization techniques.\n",
    "\n",
    "Underfitting: Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. It performs poorly on both the training and unseen data.\n",
    "\n",
    "Consequences: Poor performance, low accuracy on training and test data.\n",
    "\n",
    "Mitigation: Increase model complexity, add relevant features, use more advanced algorithms.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo reduce overfitting:\\n\\nSimplify the Model: Use simpler models with fewer parameters to reduce their capacity to memorize noise.\\nRegularization: Apply regularization techniques like L1 or L2 regularization to penalize large parameter values.\\nCross-Validation: Use techniques like k-fold cross-validation to assess model performance on multiple subsets of the data.\\nEarly Stopping: Monitor the model's performance on a validation set during training and stop when performance starts degrading.\\nFeature Selection: Select relevant features and avoid using noisy or irrelevant ones.\\nEnsemble Methods: Combine predictions from multiple models to improve generalization.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''\n",
    "To reduce overfitting:\n",
    "\n",
    "Simplify the Model: Use simpler models with fewer parameters to reduce their capacity to memorize noise.\n",
    "Regularization: Apply regularization techniques like L1 or L2 regularization to penalize large parameter values.\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess model performance on multiple subsets of the data.\n",
    "Early Stopping: Monitor the model's performance on a validation set during training and stop when performance starts degrading.\n",
    "Feature Selection: Select relevant features and avoid using noisy or irrelevant ones.\n",
    "Ensemble Methods: Combine predictions from multiple models to improve generalization.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Underfitting occurs when a model is too simplistic to capture the underlying patterns in the data. It may occur in scenarios where:\\n\\nThe model chosen is too simple for the complexity of the data.\\nInsufficient relevant features are used.\\nThe training data is too noisy or limited.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Underfitting occurs when a model is too simplistic to capture the underlying patterns in the data. It may occur in scenarios where:\n",
    "\n",
    "The model chosen is too simple for the complexity of the data.\n",
    "Insufficient relevant features are used.\n",
    "The training data is too noisy or limited.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bias-variance tradeoff represents a balance between two sources of error in a model:\\n\\n1.Bias: High bias implies the model is too simplistic and fails to capture the underlying patterns in the data. It leads to underfitting.\\n2.Variance: High variance indicates the model is too sensitive to noise in the training data. It leads to overfitting.\\n3.The relationship: As you reduce bias, variance tends to increase, and vice versa. The goal is to find the right balance that minimizes the overall error on unseen data (test data).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''The bias-variance tradeoff represents a balance between two sources of error in a model:\n",
    "\n",
    "1.Bias: High bias implies the model is too simplistic and fails to capture the underlying patterns in the data. It leads to underfitting.\n",
    "2.Variance: High variance indicates the model is too sensitive to noise in the training data. It leads to overfitting.\n",
    "3.The relationship: As you reduce bias, variance tends to increase, and vice versa. The goal is to find the right balance that minimizes the overall error on unseen data (test data).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1.Overfitting Detection: Overfitting can be detected by comparing the model's performance on the training and validation/test data. If the model performs significantly better on the training data but poorly on validation/test data, overfitting is likely.\\n2.Underfitting Detection: Underfitting can be detected if the model performs poorly on both training and validation/test data. It suggests that the model is too simple to capture the underlying patterns.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''\n",
    "1.Overfitting Detection: Overfitting can be detected by comparing the model's performance on the training and validation/test data. If the model performs significantly better on the training data but poorly on validation/test data, overfitting is likely.\n",
    "2.Underfitting Detection: Underfitting can be detected if the model performs poorly on both training and validation/test data. It suggests that the model is too simple to capture the underlying patterns.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHigh Bias (Underfitting): The model is too simplistic, and it performs poorly on both training and test data. It fails to capture the complexity of the data. Example: Linear regression applied to a highly nonlinear dataset.\\n\\nHigh Variance (Overfitting): The model fits the training data very closely, including noise, but performs poorly on test data. It doesn't generalize well. Example: A decision tree with many levels fitted to a small dataset.\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''\n",
    "High Bias (Underfitting): The model is too simplistic, and it performs poorly on both training and test data. It fails to capture the complexity of the data. Example: Linear regression applied to a highly nonlinear dataset.\n",
    "\n",
    "High Variance (Overfitting): The model fits the training data very closely, including noise, but performs poorly on test data. It doesn't generalize well. Example: A decision tree with many levels fitted to a small dataset.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Regularization is a technique to prevent overfitting by adding a penalty term to the model's loss function. Common regularization techniques include:\\n\\nL1 Regularization (Lasso): Adds the sum of the absolute values of the coefficients to the loss function. It encourages sparsity in the model.\\nL2 Regularization (Ridge): Adds the sum of the squared values of the coefficients to the loss function. It discourages large coefficient values.\\nElastic Net: Combines both L1 and L2 regularization.\\nDropout: Used in neural networks, it randomly deactivates a fraction of neurons during each training step, preventing reliance on specific neurons.\\nThese techniques constrain the model's complexity, reducing the risk of overfitting by discouraging large parameter values or encouraging sparsity.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''Regularization is a technique to prevent overfitting by adding a penalty term to the model's loss function. Common regularization techniques include:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the sum of the absolute values of the coefficients to the loss function. It encourages sparsity in the model.\n",
    "L2 Regularization (Ridge): Adds the sum of the squared values of the coefficients to the loss function. It discourages large coefficient values.\n",
    "Elastic Net: Combines both L1 and L2 regularization.\n",
    "Dropout: Used in neural networks, it randomly deactivates a fraction of neurons during each training step, preventing reliance on specific neurons.\n",
    "These techniques constrain the model's complexity, reducing the risk of overfitting by discouraging large parameter values or encouraging sparsity.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
